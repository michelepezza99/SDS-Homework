---
title: "Homework SDS"
author: "Andrea De Vincenzo, Domenico Azzarito and Michele Pezza"
date: "`r Sys.Date()`"
output:
  html_document:
    css: "styles.css"  # Link to your custom CSS file
fontsize: 12pt
geometry: margin=1in 1in 1in 1in  # Set all margins to 1 inch
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1

In this part, we work on deriving and understanding the Bayes classification rule, denoted as $\eta^*(x)$, in a simple binary classification scenario. The goal of this rule is to minimize classification errors by assigning each observation to the class with the higher posterior probability.

Here, we assume $Y \in \{0, 1\}$ is a binary outcome, and $X \in \mathbb{R}$ is a continuous feature. The conditional distributions of $X$ given $Y$ are defined as uniform distributions, and both classes have equal prior probabilities. Using this setup, we derive the regression function $r(x)$, which gives the conditional probability $\mathbb{P}(Y=1 \mid X=x)$, and use it to define the decision rule.

In this section, we break down the steps to calculate $\eta^*(x)$ by comparing the conditional densities $f_0(x)$ and $f_1(x)$ for the two classes. This leads to a clear definition of the decision boundary, showing how the optimal rule can be applied to classify observations based on their feature values. 


## 1. Bayes classification rule   $\eta^{*}(x)$
$(Y,X)$ are random variables with $Y \in \{0,1\}$ and $X \in \mathbb{R}$.
Suppose that

$$
(X \mid Y = 0) \sim \text{Unif}(-3, 1) \quad \text{and} \quad (X \mid Y = 1) \sim \text{Unif}(-1, 3)
$$

Further suppose that $\mathbb{P}(Y = 0) = \mathbb{P}(Y = 1) = \frac{1}{2}$.

---

The regression function is defined as follows:
$$
r(x) = \mathbb{E}(Y \mid X = x) = \mathbb{P}(Y=1 \mid X = x) = \dfrac{\pi_1f_1(x)}{\pi_1f_1(x) + (1-\pi_1)f_0(x)}
$$

where $\pi_1 = \mathbb{P}(Y = 1), f_1(x) = f(x \mid Y = 1) \text{ and }  f_0(x) = f(x \mid Y = 0)$.


---

The Bayes classification rule $\eta^{*}(x)$ is defined as:

$$
\eta^*(x) = 
\begin{cases} 
1 & \text{if } \mathbb{P}(Y = 1 | X = x) > \mathbb{P}(Y = 0 | X = x) \\
0 & \text{otherwise}
\end{cases} = \begin{cases}
1 & \text{if } \pi_1f_1(x) > (1-\pi_1)f_0(x) \\
0 & \text{otherwise}
\end{cases}
$$



Since we have $\pi_1 = \pi_0 = \frac{1}{2}$:
$$
\eta^*(x) = 
\begin{cases} 
1 & \text{if } f_1(x) > f_0(x) \\
0 & \text{otherwise}
\end{cases}
$$

---

In our setup:
$$
f_1(x) = 
\begin{cases} 
\frac{1}{4} & \text{if } -1 \leq x \leq 3 \\
0 & \text{otherwise}
\end{cases} 
\quad \text{and} \quad f_0(x) = 
\begin{cases} 
\frac{1}{4} & \text{if } -3 \leq x \leq 1 \\
0 & \text{otherwise}
\end{cases} 
$$

---

+ $x < -3$:
$$
f_1(x) = 0, \; f_0(x) = 0 \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
$$
+ $-3 \leq x < -1$:
$$
f_1(x) = 0, \; f_0(x) = \frac{1}{4} \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
$$
+ $-1 \leq x \leq 1$:
$$
f_1(x) = \frac{1}{4}, \; f_0(x) = \frac{1}{4} \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
$$
+ $1 < x \leq 3$:
$$
f_1(x) = \frac{1}{4}, \; f_0(x) = 0 \Rightarrow f_1(x) > f_0(x) \Rightarrow \eta^{*}(x) = 1 
$$
+ $x > 3$:
$$
f_1(x) = 0, \; f_0(x) = 0 \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0
$$


Hence, here the Bayes classification rule $\eta^{*}(x)$ is:


$$
\eta^*(x) = 
\begin{cases} 
1 & \text{if} \quad 1 < x \leq 3 \\
0 & \text{otherwise}
\end{cases}
$$

## 2.1 
Now, we generate a dataset of size $n = 1000$ from the joint data model $p(y, x) = p(x | y) \cdot p(y)$ described earlier. 
The data will include samples drawn from the specified uniform distributions for $X$ conditioned on $Y$. 
We will then visualize the generated dataset.

```{r point 1.2, echo=TRUE}
source('Functions.R')
set.seed(54) # We begin with setting the seed for reproducibility
n <- 1000 # Sample size 
# Define the distribution

# We simulate one dataset
data <- simulate(n)  # defined in 'Functions.R'

# Plot the data
plot(data$x, data$y, 
     col = ifelse(data$y == 1, adjustcolor("darkred", alpha.f = 0.1), 
                                  adjustcolor("darkblue", alpha.f = 0.1)), 
     pch = 16, cex = 0.7, xlab = "X", ylab = "Y", 
     main = "Generated dataset and Regression function", 
     sub = expression(eta(x) == 1 ~ "if and only if" ~ r(x) > 0.5))

# Plot the Regression Function r(x)
x_seq <- seq(-3, 3, length.out = 1000)
r_values <- reg_fun(x_seq)   # 

lines(x_seq, r_values, type = 's', 
      col = adjustcolor("darkgreen", alpha.f = 0.4), 
      lwd = 3) 

# Add the regression function threshold
abline(h = 0.5, lty = 3, lwd =2)

# Add Legend
legend("topleft", legend = c("Y = 0", "Y = 1", "r(x)",'threshold'), 
       col = c('darkblue','darkred','darkgreen','black'), lty = c(0, 0, 1,3), 
       pch = c(16, 16, NA,NA), pt.cex = c(0.7, 0.7, NA,NA), lwd = c(NA, NA, 2,2))
```


This visualization shows the generated dataset and the regression function $r(x)$ used to model the probability of $Y = 1$. The blue points represent observations where the outcome is 0, and the red points represent observations where the outcome is 1. The green stepwise line represents the regression function $r(x)$, which models the posterior probability $\mathbb{P}(Y = 1 | X = x)$ based on the underlying distributions defined for $X$ given $Y$. The stepwise nature of the function reflects how probabilities change sharply at the boundaries, consistent with the uniform distributions specified in the data generation process. The dotted black horizontal line at $r(x) = 0.5$ acts as a classification threshold. According to the Bayes classifier, $\eta(x) = 1$ if and only if $r(x) > 0.5$. This decision rule splits the feature space into two regions: predicted as $Y = 0$ where $r(x) \leq 0.5$ and predicted as $Y = 1$ where $r(x) > 0.5$. 

## 2.2

Now, we focus on the evaluation of the Bayes classifier.
```{r point 1.3, echo=TRUE}
# We evaluate its performance

# Predict using the Bayes classifier
data$y_pred <- bayes_classifier(data$x)     # function defined in 'Functions.R'

accuracy <- mean(data$y == data$y_pred) * 100
cat("Accuracy of Bayes Classifier: ", accuracy, "%\n",sep = '')

```

```{r, echo=TRUE}
cat("Size of Y = 0: ", sum(data$y == 0), "\n")
cat("Size of Y = 1: ", sum(data$y == 1), "\n")

```

Since the dataset is balanced, Accuracy alone is a reliable metric for evaluating the performance of the classifier.

## 2.3
Here, we consider a comparison between the Bayes classifier and the Logistic regression classifier.

Logistic regression is a supervised machine learning algorithm used for binary classification problems. It models the probability that an observation belongs to the positive class, $Y = 1$, given input features, $X_1, \ldots, X_p$. The probability is modeled using the logistic (sigmoid) function:

$$
\mathbb{P}(Y = 1 | X) = \frac{1}{1 + e^{-z}}
$$

where $z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p$ is a linear combination of input features, and $\beta$ are parameters estimated using Maximum Likelihood Estimation (MLE). Predictions are made by assigning $Y = 1$ if $\mathbb{P}(Y = 1 | X) > 0.5$ and $Y = 0$ otherwise. Logistic regression assumes a linear relationship between the features and the log-odds, independent observations, and no multicollinearity. 

To implement our logistic regression model, we use $k$-fold cross-validation to evaluate its performance. Specifically, we partition the simulated dataset into $k = 5$ folds, ensuring that each observation is used for both training and validation at least once. In each iteration, one fold serves as the validation set, while the remaining $k-1$ folds are combined to form the training set.

The logistic regression model is trained on the training set, and predictions are made on the validation set. The model's accuracy is calculated for each fold, and the process is repeated for all $k$ folds. Finally, the cross-validated accuracy is obtained by averaging the accuracies across all folds, providing a robust estimate of the model's performance.

By using $k$-fold cross-validation, we ensure a more reliable evaluation compared to a single train-test split, as all data points contribute to both training and validation, mitigating potential biases due to random partitioning.


```{r}
set.seed(54)

# Perform 5-fold cross-validation using the defined function
mean_accuracy <- cross_validate(data, k = 5)    # defined in 'Functions.R'

# Print the result
cat("Mean Cross-Validated Accuracy: ", mean_accuracy, "%\n")




```

The two accuracies — 73.3% for the Bayes classifier and 72.5% for the logistic regression model evaluated using 5-fold cross-validation — reflect the performance differences between the theoretical and practical classifiers.

The Bayes classifier achieves a slightly higher accuracy as it directly leverages the known underlying data-generating process, making it the optimal theoretical benchmark for classification in this setup. In contrast, logistic regression relies on learning the decision boundary from the data, which introduces slight variability and a minor performance drop due to the practical challenges of model estimation and potential misalignment with the true data distribution.



## 3. Repeated Sampling

In this section, we perform a repeated sampling experiment to compare the performance of the Bayes classifier and logistic regression. Specifically, we simulate $M = 10,000$ datasets, each with $n = 1,000$ observations, drawn from the specified data-generating process. 

For each simulated dataset:

1. The Bayes classifier is applied to the entire dataset, and its accuracy is recorded. This serves as the theoretical benchmark since it directly leverages the known underlying distributions.

2. Logistic regression is evaluated using 5-fold cross-validation on the same dataset. The cross-validated accuracy is recorded to represent the practical performance of a data-driven classifier.

After $M$ repetitions, we calculate the mean and standard deviation of accuracies for both classifiers. 


```{r, echo=TRUE}
# set.seed(54)
# M <- 10000  # Number of repetitions
# n <- 1000   # Sample size
# 
# # Initialize vectors to store accuracies
# accuracy_bayes <- numeric(M)
# accuracy_logistic <- numeric(M)
# 
# # Loop for repeated sampling
# for (i in 1:M) {
#   # Generate new dataset
#   data <- simulate(n)
#   
#   # Bayes Classifier: Evaluate on all data
#   data$y_pred <- bayes_classifier(data$x)
#   accuracy_bayes[i] <- mean(data$y == data$y_pred)
#   
#   # Logistic Regression: Use cross-validation
#   accuracy_logistic[i] <- cross_validate(data, k = 5)
# }
# 
# # Compute summary statistics
# mean_bayes <- mean(accuracy_bayes) * 100
# mean_logistic <- mean(accuracy_logistic) 
# sd_bayes <- sd(accuracy_bayes) * 100
# sd_logistic <- sd(accuracy_logistic) 
# 
# cat("Bayes Classifier - Mean Accuracy: ", mean_bayes, "%, Std Dev: ", sd_bayes, "%\n")
# cat("logistic regression - mean accuracy: ", mean_logistic, "%, std dev: ", sd_logistic, "%\n")



```

The results of the repeated sampling experiment show that logistic regression slightly outperforms the Bayes classifier. The **Bayes classifier** achieves a mean accuracy of approximately 74.98% with a standard deviation of 1.38%, while **logistic regression**, evaluated using 5-fold cross-validation, attains a slightly higher mean accuracy of 75.02% with a lower standard deviation of 1.37%.

The slight edge in performance for logistic regression may seem counterintuitive, as the Bayes classifier is theoretically optimal under the assumption that the data-generating process is perfectly known. However, this outcome highlights an important distinction: while the Bayes classifier relies entirely on the assumed uniform distributions for $X \mid Y$, logistic regression leverages the data itself to learn the decision boundary. This data-driven approach allows logistic regression to adapt to small deviations or noise in the simulated data that might not perfectly align with the assumed theoretical distributions.

Moreover, the cross-validation process used for logistic regression helps mitigate overfitting by effectively utilizing the dataset to generalize better across folds. This adaptability gives logistic regression a practical advantage in real-world scenarios where the true underlying distribution is unknown or subject to variability.

Overall, these results underscore the power of logistic regression as a flexible and robust classifier, capable of achieving performance comparable to or even surpassing theoretical benchmarks like the Bayes classifier in applied settings.


# Part 2


In this section, we aim to summarize and present the core methodology described by Jerome H. Friedman in his paper on multivariate goodness-of-fit and two-sample testing. Our approach is to distill the key steps into a clear and well-structured pseudo-code, providing a high-level overview of the testing procedure.

We begin by outlining how to generate and process the datasets, followed by leveraging a binary classifier to assign scores that differentiate the two samples. These scores are then analyzed using univariate two-sample tests, such as Mann-Whitney or Kolmogorov-Smirnov, which act as baseline methods for assessing distributional differences. Finally, we describe how the null hypothesis is tested and interpreted to draw meaningful conclusions.

Our goal is to highlight how machine learning techniques, particularly binary classifiers, can be effectively combined with traditional statistical methods to address complex multivariate testing problems. This pseudo-code is intended to serve as a guide for implementing the procedure in practice while ensuring clarity and precision.


## Pseudo-code for Friedman's paper

---

1. Generate Datasets:
   - Start with two datasets:
     - Sample 1: $\{x_i\}$ with $N$ observations drawn from $p(x)$.
     - Sample 2: $\{z_i\}$ with $M$ observations drawn from $q(x)$.

2. Combine the Datasets:
   - Merge the two datasets into one:
     - $\{u_i\} = \{x_i\} \cup \{z_i\}$.
   - Assign labels to identify the origin of each observation:
     - Assign $y_i = 1$ for data points from Sample 1 ($\{x_i\}$).
     - Assign $y_i = -1$ for data points from Sample 2 ($\{z_i\}$).

3. Train a Binary Classifier:
   - Use a machine learning model (e.g. logistic regression) to classify the combined data.
   - Input: The dataset $\{u_i\}$.
   - Output: Scores $\{s_i\}$ for each data point, where $s_i = F(u_i)$ is the score assigned by the classifier.

4. Compute Scores for Each Sample:
   - Split the scores into two groups based on their original datasets:
     - $S^+ = \{s_i \, \text{for all } x_i \, \text{(Sample 1)}\}$.
     - $S^- = \{s_i \, \text{for all } z_i \, \text{(Sample 2)}\}$.

5. Perform a Two-Sample Test:
   - Apply a univariate two-sample test (like Mann-Whitney or Kolmogorov-Smirnov) on the scores:
     - Compute the test statistic: $t = T(S^+, S^-)$.

6. Test the Null Hypothesis:
   - Estimate the null distribution for the test statistic:
     - If different datasets are used for training and scoring:
       - Use the standard null distribution (e.g., for Mann-Whitney or Kolmogorov-Smirnov).
     - If the same dataset is used for both training and scoring:
       - Use a permutation test to create the null distribution.
   - Compare the observed test statistic $t$ to the critical value for a chosen significance level $\alpha$.
   - Reject the null hypothesis ($p = q$) if $t$ is greater than the critical value.

7. Draw a Conclusion:
   - Based on the significance test, decide whether to reject or fail to reject the null hypothesis.
   - If rejected, conclude that the two samples are likely drawn from different distributions.

---

Friedman suggests using the **Mann-Whitney** and **Kolmogorov-Smirnov** (KS) tests as baseline methods because they are simple, reliable, and effective for comparing two distributions. Both tests are **univariate** and **non-parametric**, which means they don’t rely on the data following a specific distribution. This makes them flexible and applicable in many different situations.

The Mann-Whitney test is particularly helpful when we want to compare the central tendencies, such as the medians, of two groups. It works by ranking all the observations and comparing the ranks between the two samples, making it ideal for identifying whether the central values of the groups differ. In contrast, the KS test looks at the overall shape of the distributions by comparing their cumulative distribution functions (CDFs). This allows it to detect differences in not just central tendencies but also in spread, shape, or location, providing a broader perspective.

These tests are great starting points because they are **easy to understand and implement**. They don’t require complicated calculations, and their **non-parametric nature** makes them adaptable to different types of data **without needing strict assumptions**. Since Friedman's method generates univariate scores from the classifier, these tests are perfectly suited to evaluate those scores, ensuring they align well with the problem at hand.




```{r}

```


