---
title: "Homework-SDS"
author: "Andrea De Vincenzo, Domenico Azzarito and Michele Pezza"
date: "`r Sys.Date()`"
output:
  html_document:
    css: "styles.css"  # Link to your custom CSS file
fontsize: 12pt
geometry: margin=1in 1in 1in 1in  # Set all margins to 1 inch
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1

In this part, we work on deriving and understanding the Bayes
classification rule, denoted as $\eta^*(x)$, in a simple binary
classification scenario. The goal of this rule is to minimize
classification errors by assigning each observation to the class with
the higher posterior probability.

Here, we assume $Y \in \{0, 1\}$ is a binary outcome, and
$X \in \mathbb{R}$ is a continuous feature. The conditional
distributions of $X$ given $Y$ are defined as uniform distributions, and
both classes have equal prior probabilities. Using this setup, we derive
the regression function $r(x)$, which gives the conditional probability
$\mathbb{P}(Y=1 \mid X=x)$, and use it to define the decision rule.

In this section, we break down the steps to calculate $\eta^*(x)$ by
comparing the conditional densities $f_0(x)$ and $f_1(x)$ for the two
classes. This leads to a clear definition of the decision boundary,
showing how the optimal rule can be applied to classify observations
based on their feature values.

## 1. Bayes classification rule $\eta^{*}(x)$

$(Y,X)$ are random variables with $Y \in \{0,1\}$ and
$X \in \mathbb{R}$. Suppose that

$$
(X \mid Y = 0) \sim \text{Unif}(-3, 1) \quad \text{and} \quad (X \mid Y = 1) \sim \text{Unif}(-1, 3)
$$

Further suppose that
$\mathbb{P}(Y = 0) = \mathbb{P}(Y = 1) = \frac{1}{2}$.

------------------------------------------------------------------------

The regression function is defined as follows: $$
r(x) = \mathbb{E}(Y \mid X = x) = \mathbb{P}(Y=1 \mid X = x) = \dfrac{\pi_1f_1(x)}{\pi_1f_1(x) + (1-\pi_1)f_0(x)}
$$

where
$\pi_1 = \mathbb{P}(Y = 1), f_1(x) = f(x \mid Y = 1) \text{ and }  f_0(x) = f(x \mid Y = 0)$.

------------------------------------------------------------------------

The Bayes classification rule $\eta^{*}(x)$ is defined as:

$$
\eta^*(x) = 
\begin{cases} 
1 & \text{if } \mathbb{P}(Y = 1 | X = x) > \mathbb{P}(Y = 0 | X = x) \\
0 & \text{otherwise}
\end{cases} = \begin{cases}
1 & \text{if } \pi_1f_1(x) > (1-\pi_1)f_0(x) \\
0 & \text{otherwise}
\end{cases}
$$

Since we have $\pi_1 = \pi_0 = \frac{1}{2}$: $$
\eta^*(x) = 
\begin{cases} 
1 & \text{if } f_1(x) > f_0(x) \\
0 & \text{otherwise}
\end{cases}
$$

------------------------------------------------------------------------

In our setup: $$
f_1(x) = 
\begin{cases} 
\frac{1}{4} & \text{if } -1 \leq x \leq 3 \\
0 & \text{otherwise}
\end{cases} 
\quad \text{and} \quad f_0(x) = 
\begin{cases} 
\frac{1}{4} & \text{if } -3 \leq x \leq 1 \\
0 & \text{otherwise}
\end{cases} 
$$

------------------------------------------------------------------------

-   $x < -3$: $$
    f_1(x) = 0, \; f_0(x) = 0 \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
    $$
-   $-3 \leq x < -1$: $$
    f_1(x) = 0, \; f_0(x) = \frac{1}{4} \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
    $$
-   $-1 \leq x \leq 1$: $$
    f_1(x) = \frac{1}{4}, \; f_0(x) = \frac{1}{4} \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0 
    $$
-   $1 < x \leq 3$: $$
    f_1(x) = \frac{1}{4}, \; f_0(x) = 0 \Rightarrow f_1(x) > f_0(x) \Rightarrow \eta^{*}(x) = 1 
    $$
-   $x > 3$: $$
    f_1(x) = 0, \; f_0(x) = 0 \Rightarrow f_1(x) \not> f_0(x) \Rightarrow \eta^{*}(x) = 0
    $$

Hence, here the Bayes classification rule $\eta^{*}(x)$ is:

$$
\eta^*(x) = 
\begin{cases} 
1 & \text{if} \quad 1 < x \leq 3 \\
0 & \text{otherwise}
\end{cases}
$$

## 2.1

Now, we generate a dataset of size $n = 1000$ from the joint data model
$p(y, x) = p(x | y) \cdot p(y)$ described earlier. The data will include
samples drawn from the specified uniform distributions for $X$
conditioned on $Y$. We will then visualize the generated dataset.

```{r point 1.2, echo=TRUE}
source('Functions.R')
set.seed(54) # We begin with setting the seed for reproducibility
n <- 1000 # Sample size 
# Define the distribution

# We simulate one dataset
data <- simulate(n)  # defined in 'Functions.R'

# Plot the data
plot(data$x, data$y, 
     col = ifelse(data$y == 1, adjustcolor("darkred", alpha.f = 0.1), 
                                  adjustcolor("darkblue", alpha.f = 0.1)), 
     pch = 16, cex = 0.7, xlab = "X", ylab = "Y", 
     main = "Generated dataset and Regression function", 
     sub = expression(eta(x) == 1 ~ "if and only if" ~ r(x) > 0.5))

# Plot the Regression Function r(x)
x_seq <- seq(-3, 3, length.out = 1000)
r_values <- reg_fun(x_seq)   # 

lines(x_seq, r_values, type = 's', 
      col = adjustcolor("darkgreen", alpha.f = 0.4), 
      lwd = 3) 

# Add the regression function threshold
abline(h = 0.5, lty = 3, lwd =2)

# Add Legend
legend("topleft", legend = c("Y = 0", "Y = 1", "r(x)",'threshold'), 
       col = c('darkblue','darkred','darkgreen','black'), lty = c(0, 0, 1,3), 
       pch = c(16, 16, NA,NA), pt.cex = c(0.7, 0.7, NA,NA), lwd = c(NA, NA, 2,2))
```

This visualization shows the generated dataset and the regression
function $r(x)$ used to model the probability of $Y = 1$. The blue
points represent observations where the outcome is 0, and the red points
represent observations where the outcome is 1. The green stepwise line
represents the regression function $r(x)$, which models the posterior
probability $\mathbb{P}(Y = 1 | X = x)$ based on the underlying
distributions defined for $X$ given $Y$. The stepwise nature of the
function reflects how probabilities change sharply at the boundaries,
consistent with the uniform distributions specified in the data
generation process. The dotted black horizontal line at $r(x) = 0.5$
acts as a classification threshold. According to the Bayes classifier,
$\eta(x) = 1$ if and only if $r(x) > 0.5$. This decision rule splits the
feature space into two regions: predicted as $Y = 0$ where
$r(x) \leq 0.5$ and predicted as $Y = 1$ where $r(x) > 0.5$.

## 2.2

Now, we focus on the evaluation of the Bayes classifier.

```{r point 1.3, echo=TRUE}
# We evaluate its performance

# Predict using the Bayes classifier
data$y_pred <- bayes_classifier(data$x)     # function defined in 'Functions.R'

accuracy <- mean(data$y == data$y_pred) * 100
cat("Accuracy of Bayes Classifier: ", accuracy, "%\n",sep = '')

```

```{r, echo=TRUE}
cat("Size of Y = 0: ", sum(data$y == 0), "\n")
cat("Size of Y = 1: ", sum(data$y == 1), "\n")

```

Since the dataset is balanced, Accuracy alone is a reliable metric for
evaluating the performance of the classifier.

## 2.3

Here, we consider a comparison between the Bayes classifier and the
Logistic regression classifier.

Logistic regression is a supervised machine learning algorithm used for
binary classification problems. It models the probability that an
observation belongs to the positive class, $Y = 1$, given input
features, $X_1, \ldots, X_p$. The probability is modeled using the
logistic (sigmoid) function:

$$
\mathbb{P}(Y = 1 | X) = \frac{1}{1 + e^{-z}}
$$

where $z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p$ is
a linear combination of input features, and $\beta$ are parameters
estimated using Maximum Likelihood Estimation (MLE). Predictions are
made by assigning $Y = 1$ if $\mathbb{P}(Y = 1 | X) > 0.5$ and $Y = 0$
otherwise. Logistic regression assumes a linear relationship between the
features and the log-odds, independent observations, and no
multicollinearity.



```{r}
set.seed(54)

# Train Set
train_data <- data

# Test Set
m = 500
test_data <- simulate(m)

# Logistic Regression Model
logistic_model <- glm(y ~ x, data = train_data, family = binomial)

# We predict probabilities on the test dataset
test_data$y_prob <- predict(logistic_model, newdata = test_data, type = "response")

#  And convert those probabilities to binary predictions using a 0.5 threshold
test_data$y_pred <- ifelse(test_data$y_prob > 0.5, 1, 0)

# Compute accuracy
test_accuracy <- mean(test_data$y == test_data$y_pred) * 100
cat("Test Accuracy of Logistic Regression: ", test_accuracy, "%\n")



```
Now, we evaluate the Bayes classifier on the test set:
```{r}
# Apply the Bayes classifier on the test set
test_data$bayes_pred <- bayes_classifier(test_data$x)  # Function must be defined in 'Functions.R'

# Compute Accuracy for Bayes Classifier
bayes_accuracy <- mean(test_data$y == test_data$bayes_pred) * 100
cat("Test Accuracy of Bayes Classifier:", bayes_accuracy, "%\n")

# Compare with Logistic Regression
cat("Test Accuracy of Logistic Regression:", accuracy, "%\n")

```
The accuracy results (75.4% for the Bayes classifier and 73.3% for the logistic regression model) highlight the performance gap between a theoretical and a practical classifier.

The Bayes classifier achieves slightly higher accuracy as it directly leverages the true underlying data-generating process, serving as the optimal benchmark for classification in this setup. In contrast, logistic regression estimates the decision boundary from the data, introducing variability and a minor performance drop due to practical challenges in model estimation and potential misalignment with the true data distribution.

```{r}
# We define a sequence of x values to generate predictions over a smooth range,
# ensuring a detailed representation of the regression function.
x_seq <- seq(min(data$x), max(data$x), length.out = 1000)

# We compute the predicted probabilities using our logistic regression model.
# This allows us to visualize how the model estimates the probability of Y = 1 across the x-axis.
logistic_prob <- predict(logistic_model, newdata = data.frame(x = x_seq), type = "response")

# We plot the dataset, differentiating between the two classes.
# Points corresponding to Y = 1 are shown in a semi-transparent dark red,
# while points for Y = 0 are displayed in a semi-transparent dark blue.
plot(data$x, data$y, 
     col = ifelse(data$y == 1, adjustcolor("darkred", alpha.f = 0.1), 
                                  adjustcolor("darkblue", alpha.f = 0.1)), 
     pch = 16, cex = 0.7, xlab = "X", ylab = "Y", 
     main = "Generated Dataset and Regression Function", 
     sub = expression(eta(x) == 1 ~ "if and only if" ~ r(x) > 0.5))

# We compute the Bayes regression function, which represents the theoretical 
# optimal probability of Y = 1 given X. This function should be pre-defined in 'Functions.R'.
r_values <- reg_fun(x_seq)

# We overlay the Bayes regression function onto the plot using a step function.
# This serves as the optimal decision boundary for classification.
lines(x_seq, r_values, type = 's', col = adjustcolor("darkgreen", alpha.f = 0.6), lwd = 3) 

# We add the estimated probabilities from the logistic regression model.
# This allows us to compare our logistic model's estimated function against the Bayes optimal function.
lines(x_seq, logistic_prob, col = "purple", lwd = 3, lty = 2)

# We add a legend to clarify the different elements present in the plot.
# - Y = 0 and Y = 1 represent the data points.
# - The Bayes Regression Function represents the optimal probability function.
# - The Logistic Regression Estimate shows the model's learned probabilities.
legend('bottomright', 
       legend = c("Y = 0", "Y = 1", "Bayes Regression Function", "Logistic Regression Estimate"),
       col = c("darkblue", "darkred", "darkgreen", "purple"), 
       lty = c(NA, NA, 1, 2), 
       pch = c(16, 16, NA, NA), 
       pt.cex = c(1.2, 1.2, NA, NA),  # We increase the point size for better visibility.
       lwd = c(NA, NA, 3, 3),          # We ensure the regression function lines are clearly visible.
       bty = "n")  # We remove the legend box for a cleaner aesthetic.



```

The Bayes function represents the **optimal classifier**, showing the true probability \( P(Y = 1 \mid X) \) with a **stepwise shape**, reflecting an abrupt decision boundary. In contrast, logistic regression estimates probabilities using a **smooth, sigmoidal function**, making it a practical but imperfect approximation. The dataset shows a **clear separation** between classes, making classification straightforward. While the Bayes function is **ideal but unknown**, logistic regression is **robust** and generalizes well, though it may struggle with sharp decision boundaries. This highlights the trade-off between **theoretical optimality and practical modeling**, as logistic regression smooths transitions that might be abrupt in reality.

## 3. Repeated Sampling

In this section, we conduct a repeated sampling experiment to compare the performance of the Bayes classifier and logistic regression. Specifically, we simulate $M = 10,000$ datasets, each consisting of a training set with \( n = 1,000 \) observations and a test set with $m = 500$ observations, drawn from the specified data-generating process.  

For each simulated dataset:  

1. **The Bayes classifier** is applied to the test set, and its accuracy is recorded. This serves as the theoretical benchmark since it directly utilizes the known underlying distributions.  

2. **Logistic regression** is trained on the training set and evaluated on the test set. The accuracy of its predictions on the test set is recorded, representing the practical performance of a data-driven classifier.  

After $M$ repetitions, we compute the mean and standard deviation of the accuracies for both classifiers to assess their overall performance.  

```{r, echo=TRUE}
set.seed(78)  # Ensure reproducibility
M <- 10000  # Number of iterations
n <- 1000   # Train set size
m <- 500    # Test set size
bayes_accuracies <- numeric(M)
logistic_accuracies <- numeric(M)

for (i in 1:M) {
  # Generate training dataset
  train_data <- simulate(n)
  
  # Train Logistic Regression
  logistic_model <- glm(y ~ x, data = train_data, family = binomial)
  
  # Generate test dataset (new independent sample)
  test_data <- simulate(m)
  
  # Bayes Classifier Predictions on test data
  test_data$y_pred_bayes <- bayes_classifier(test_data$x)
  bayes_accuracies[i] <- mean(test_data$y == test_data$y_pred_bayes)
  
  # Logistic Regression Predictions on test data
  prob_pred <- predict(logistic_model, newdata = test_data, type = "response")
  test_data$y_pred_logistic <- ifelse(prob_pred > 0.5, 1, 0)
  logistic_accuracies[i] <- mean(test_data$y == test_data$y_pred_logistic)
}

# Compute average accuracies
bayes_mean_acc <- mean(bayes_accuracies) * 100
logistic_mean_acc <- mean(logistic_accuracies) * 100

cat("Average Accuracy over", M, "iterations:\n")
cat("Bayes Classifier:", bayes_mean_acc, "%\n")
cat("Logistic Regression:", logistic_mean_acc, "%\n")

# Compare Performance
if (bayes_mean_acc > logistic_mean_acc) {
  cat("Bayes Classifier performs better on average.\n")
} else if (logistic_mean_acc > bayes_mean_acc) {
  cat("Logistic Regression performs better on average.\n")
} else {
  cat("Both classifiers perform equally well on average.\n")
}




```
The results of the repeated sampling experiment indicate that logistic regression marginally outperforms the Bayes classifier. The **Bayes classifier** achieves a mean accuracy of approximately 75.03%, while **logistic regression** attains a slightly higher mean accuracy of 75.04%.  

At first glance, this slight advantage for logistic regression may seem unexpected, given that the Bayes classifier is theoretically optimal under the assumption of a perfectly known data-generating process. However, this result highlights an important practical consideration: while the Bayes classifier relies entirely on the assumed distributions of $X \mid Y$, logistic regression learns the decision boundary directly from the data. This flexibility allows logistic regression to adapt to small variations or noise in the simulated datasets that may not perfectly conform to the theoretical assumptions.  

Additionally, the data-driven nature of logistic regression enables it to generalize effectively, even in cases where the underlying distributions are subject to slight deviations. This adaptability gives logistic regression a practical advantage in real-world applications, where the true data distribution is often unknown or imperfectly specified.  

Overall, these findings reinforce the robustness of logistic regression as a versatile classifier, capable of performing on par with or even slightly surpassing theoretical benchmarks like the Bayes classifier in applied settings.  

# Part 2

In this section, we aim to summarize and present the core methodology
described by Jerome H. Friedman in his paper on multivariate
goodness-of-fit and two-sample testing. Our approach is to distill the
key steps into a clear and well-structured pseudo-code, providing a
high-level overview of the testing procedure.

We begin by outlining how to generate and process the datasets, followed
by leveraging a binary classifier to assign scores that differentiate
the two samples. These scores are then analyzed using univariate
two-sample tests, such as Mann-Whitney or Kolmogorov-Smirnov, which act
as baseline methods for assessing distributional differences. Finally,
we describe how the null hypothesis is tested and interpreted to draw
meaningful conclusions.

Our goal is to highlight how machine learning techniques, particularly
binary classifiers, can be effectively combined with traditional
statistical methods to address complex multivariate testing problems.
This pseudo-code is intended to serve as a guide for implementing the
procedure in practice while ensuring clarity and precision.

## Pseudo-code for Friedman's paper

------------------------------------------------------------------------

1.  Generate Datasets:
    -   Start with two datasets:
        -   Sample 1: $\{x_i\}$ with $N$ observations drawn from $p(x)$.
        -   Sample 2: $\{z_i\}$ with $M$ observations drawn from $q(x)$.
2.  Combine the Datasets:
    -   Merge the two datasets into one:
        -   $\{u_i\} = \{x_i\} \cup \{z_i\}$.
    -   Assign labels to identify the origin of each observation:
        -   Assign $y_i = 1$ for data points from Sample 1 ($\{x_i\}$).
        -   Assign $y_i = -1$ for data points from Sample 2 ($\{z_i\}$).
3.  Train a Binary Classifier:
    -   Use a machine learning model (e.g. logistic regression) to
        classify the combined data.
    -   Input: The dataset $\{u_i\}$.
    -   Output: Scores $\{s_i\}$ for each data point, where
        $s_i = F(u_i)$ is the score assigned by the classifier.
4.  Compute Scores for Each Sample:
    -   Split the scores into two groups based on their original
        datasets:
        -   $S^+ = \{s_i \, \text{for all } x_i \, \text{(Sample 1)}\}$.
        -   $S^- = \{s_i \, \text{for all } z_i \, \text{(Sample 2)}\}$.
5.  Perform a Two-Sample Test:
    -   Apply a univariate two-sample test (like Mann-Whitney or
        Kolmogorov-Smirnov) on the scores:
        -   Compute the test statistic: $t = T(S^+, S^-)$.
6.  Test the Null Hypothesis:
    -   Estimate the null distribution for the test statistic:
        -   If different datasets are used for training and scoring:
            -   Use the standard null distribution (e.g., for
                Mann-Whitney or Kolmogorov-Smirnov).
        -   If the same dataset is used for both training and scoring:
            -   Use a permutation test to create the null distribution.
    -   Compare the observed test statistic $t$ to the critical value
        for a chosen significance level $\alpha$.
    -   Reject the null hypothesis ($p = q$) if $t$ is greater than the
        critical value.
7.  Draw a Conclusion:
    -   Based on the significance test, decide whether to reject or fail
        to reject the null hypothesis.
    -   If rejected, conclude that the two samples are likely drawn from
        different distributions.

------------------------------------------------------------------------

Friedman suggests using the **Mann-Whitney** and **Kolmogorov-Smirnov**
(KS) tests as baseline methods because they are simple, reliable, and
effective for comparing two distributions. Both tests are **univariate**
and **non-parametric**, which means they don’t rely on the data
following a specific distribution. This makes them flexible and
applicable in many different situations.

The Mann-Whitney test is particularly helpful when we want to compare
the central tendencies, such as the medians, of two groups. It works by
ranking all the observations and comparing the ranks between the two
samples, making it ideal for identifying whether the central values of
the groups differ. In contrast, the KS test looks at the overall shape
of the distributions by comparing their cumulative distribution
functions (CDFs). This allows it to detect differences in not just
central tendencies but also in spread, shape, or location, providing a
broader perspective.

These tests are great starting points because they are **easy to
understand and implement**. They don’t require complicated calculations,
and their **non-parametric nature** makes them adaptable to different
types of data **without needing strict assumptions**. Since Friedman's
method generates univariate scores from the classifier, these tests are
perfectly suited to evaluate those scores, ensuring they align well with
the problem at hand.

```{r}

```
